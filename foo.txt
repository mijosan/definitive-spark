스파크는 셔플의 결과를 디스크에 저장합니다.
물리적 실행 계획으로 최종 컴파일 (ex dataframe의 조건절 푸시다운;아주 복잡한 스파크잡이 원시 데이터에서 하나의 로우만 가져오는 필터를 가지고 있다면 필요한 레코드 하나만 읽는 것이 가장 효율적
스파크는 이 필터를 데이터소스로 위임 하는 최적화 작업을 자동으로 수행)
액션을 지정하면 스파크 잡이 시작됩니다.
스파크 UI는 드라이버 노드의 4040 포트로 접속(master 노드)
스파크 잡은 개별 액션에 의해 트리거되는 다수의 트랜스포메이션으로 이루어져 있다.
데이터는 SparkSession의 DataFrameReader 클래스를 사용해서 읽습니다.
실행 계획은 디버깅과 스파크의 실행 과정을 이해하는 데 도움을 주는 도구일 뿐
스파크 SQL을 사용하면 모든 DataFrame을 테이블이나 뷰(임시 테이블)로 등록한 후 SQL쿼리를 사용할 수 있습니다.(createOrReplaceTempView, sparkSession.sql())
스파크는 빅데이터 문제를 빠르게 해결하는 데 필요한 수백 개의 함수를 제공
실행 계획은 트랜스포메이션의 지향성 비순환 그래프이며 액션이 호출되며 결과를 만들어냅니다.
그리고 지향성 비순환 그래프의 각 단계는 불변성을 가진 신규 DataFrame을 생성
데이터를 드라이버로 모으는 대신 스파크가 지원하는 여러 데이터소스로 내보낼 수도 있습니다.
예를 들어 postgreSQL과 같은 데이터베이스나 다양한 포맷의 파일에 결과를 저장할 수 있습니다.
spark-submit 명령을 사용해 대화형 셸에서 개발한 프로그램을 운영용 애플리케이션으로 쉽게 전환할 수 있습니다.
스파크 애플리케이션은 스탠드얼론, 메소스 그리고 YARN 클러스터 매니저를 이용해 실행됩니다.
구조적 스트리밍은 프로토타입을 배치 잡으로 개발한 다음 스트리밍 잡으로 변환할 수 있으므로 개념잡기가 수월하다.
셔플 파티션수는 셔플 이후에 생성될 파티션 수를 의미
DataFrame과 Dataset (분산컬렉션)은 결과를 생성하기 위해 어떤 데이터에 어떤 연산을 적용해야 하는지 정의하는 지연 연산의 실행 계획이며, 불변성을 가집니다.
스키마는 DataFrame의 컬럼명과 데이터 타입을 정의. 스키마는 데이터소스에서 얻거나(스키마 온 리드;schema-on-read)
스키마는 여러 데이터 타입으로 구성되므로 어떤 데이터 타입이 어느 위치에 있는지 정의하는 방법이 필요
스파크는 실행 계획 수립과 처리에 사용하는 자체 데이터 타입 정보를 가지고 있는 카탈리스트 엔진을 사용합니다.
스파크는 자체 데이터 타입을 지원하는 여러 언어 API와 직접 매핑되며, 각 언어에 대한 매핑 테이블을 가지고 있습니다.
스파크의 구조적 API를 사용하더라도 대부분의 연산은 파이썬이나 R의 데이터 타입이 아닌 스파크의 데이터 타입을 사용
스파크에서 덧셈 연산이 수행되는 이유는 스파크가 지원하는 언어를 이용해 작성된 표현식을 카탈리스트 엔진에서
스파크의 데이터타입으로 변환해 명령을 처리하기 때문
칼럼은 정수형이나 문자열 같은 단순 데이터 타입, 배열이나 맵 같은 복합 데이터 타입 그리고 null값을 표현
DataFrame의 레코드는 Row 타입으로 구성 (spark.range(2).collect() -> Row 객체로 이루어진 배열을 반환)

# 구조적 API의 실행 과정 (스파크 코드가 클러스터에서 실제 처리되는 과정)
사용자 코드에서 실제 실행 코드로 변환되는 과정
1. DataFrame/Dataset/SQL을 이용해 코드를 작성
2. 정상적인 코드라면 스파크가 논리적 실행 계획으로 변환합니다.
3. 스파크는 논리적 실행 계획을 물리적 실행 계획으로 변환하며 그 과정에서 추가적인 최적화를 할 수 있는지 확인
4. 스파크는 클러스터에서 물리적 실행 계획(RDD 처리)을 실행합니다.

코드 작성 -> 콘솔이나 spark-submit 셸 스크립트로 실행 -> 카탈리스트 옵티마이저는 코드를 넘겨받고 실제 실행 계획을 생성
-> 마지막으로 스파크는 코드(task?)를 실행한 후 결과를 반환

물리적 실행 계획은 논리적 실행 계획을 클러스터 환경에서 실행하는 방법을 정의하는
물리적 실행 계획은 일련의 RDD와 트랜스포메이션으로 변환됩니다. 스파크는 DataFrame으로 정의된 쿼리를
RDD 트랜스포메이션으로 컴파일합니다. 따라서 스파크를 컴파일러 라고 부르기도 합니다.

실행
스파크는 물리적 실행 계획을 선정한 다음 저수준 프로그래밍 인터페이스인 RDD를 대상으로 모든 코드를 실행
스파크는 런타임에 전체 태스크나 스테이지를 제거할 수 있는 자바 바이트 코드를 생성해 추가적인 최적화를 수행되는
DataFrame의 파티셔닝은 DataFrame이 클러스터에서 물리적으로 배치되는 형태를 정의한다.
파티셔닝 스키마는 파티션을 배치하는 방법을 정의합니다.
파티셔닝의 분할 기준은 특정 칼럼이나 비결정론적값을 기반으로 설정할수있다.

칼럼은 칼럼명을 카탈로그에 저장된 정보와 비교하기 전까지 미확인 상태로 남습니다.
분석기가 동작하는 단계에서 칼럼과 테이블을 분석

SQL의 select 구문에 이전 표현식을 사용해도 잘 동작하며 동일한 결과를 생성합니다.
그 이유는 SQL 표현식과 위 예제의 DataFrame 코드는 실행 시점에 동일한 논리 트리로 컴파일 되기 때문입니다.
따라서 DataFrame 코드나 SQL로 표현식을 작성할 수 있으며 동일한 성능을 발휘
칼럼이나 표현식 사용 -> select 메서드
문자열 표현식을 사용 -> selectExpr

스파크는 자동으로 필터의 순서와 상관없이 동시에 모든 필터링 작업을 수행하기 때문에 항상 유용한것은 아니다.
그러므로 여러 개의 AND 필터를 지정하려면 차례대로 필터를 연결하고 판단은 스파크에게 맡겨야한다.

repartition 메서드를 호출하면 무조건 전체 데이터를 셔플합니다. 향후에 사용할 파티션 수가 현재 파티션
수보다 많거나 칼럼을 만드는 경우에만 사용해야 한다.

# 특정 칼럼을 기준으로 자주 필터링한다면 자주 필터링되는 칼럼을 기준으로 파티션을 재분배 하는 것이 좋다
선택적으로 파티션 수를 지정할 수도 있다(df.repartition(5 , col("DEST_COUNTRY_NAME")))
아래 출력을 섹션 1과 비교하면 파티션 3이 2로 이동되고 파티션 6이 5로 이동하여 결과적으로 2개의 파티션에서 데이터가 이동되었음을 알 수 있습니다.
Partition 1 : 0 1 2
Partition 2 : 3 4 5 6 7 8 9
Partition 4 : 10 11 12 
Partition 5 : 13 14 15 16 17 18 19
Spark DataFrame coalesce() 은 파티션 수를 줄이는 데만 사용됩니다. 이것은 병합을 사용하여 파티션 간 데이터 이동이 적은 최적화 또는 개선된 버전의 repartition()입니다.

스파크는 드라이버에서 클러스터 상태 정보를 유지합니다. 로컬 환경에서 데이터를 다루려면
드라이버로 데이터를 수집해야 합니다. (collect, take, show)


collect, toLocalIterator 메서드를 사용할 떄 매우 큰 파티션이 있다면 드라이버와 애플리케이션이 비정상적으로
종료될 수 있습니다. 또한 연산을 병렬로 수행하지 않고 차례로 처리하기 때문에 매우 큰 비용이 발생한다.
pyspark의 lit method는 literal value로 column을 생성할 때 사용됩니다.